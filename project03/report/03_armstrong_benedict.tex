\documentclass[unicode,11pt,a4paper,oneside,numbers=endperiod,openany]{scrartcl}

\usepackage{minted}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{subcaption}

\input{assignment.sty}

\begin{document}


\setassignment
\setduedate{Monday 15 April 2024, 23:59 (midnight)}

\serieheader{High-Performance Computing Lab for CSE}{2024}
{Student: Benedict Armstrong}
{Discussed with: Tristan Gabl}{Solution for Project 3}{}
\newline


\section{Implementing the linear algebra functions and the stencil
  operators}

\subsection{Linalg functions}
Implementing the eight linalg function outlined in \texttt{linalg.cpp} was relatively straighforward. Each followed a similar pattern of component wise iteration over the input array(s) and then performing the required operation. An example of the \texttt{copy} function is shown in Listing \ref{lst:linalg_copy}.

\begin{listing}[h!t]
    \begin{minted}{cpp}
    for (int i = 0; i < N; i++)
    {
        y[i] = x[i];
    }
    \end{minted}
    \caption{Linalg copy function}
    \label{lst:linalg_copy}
\end{listing}

\subsection{Stencil operators}

The next task was Implementing the stencil operator. Listing \ref{lst:stencil_operator} shows how we calculate the value for each grid cell.

\begin{listing}[h!t]
    \begin{minted}{cpp}
    f(i, j) = -(4. + alpha) * s_new(i, j) 
            + s_new(i - 1, j) + s_new(i + 1, j) 
            + s_new(i, j - 1) + s_new(i, j + 1) 
            + beta * s_new(i, j) * (1.0 - s_new(i, j)) 
            + alpha * s_old(i, j);
    \end{minted}
    \caption{Stencil operator}
    \label{lst:stencil_operator}
\end{listing}

\subsection{Plotting the results}

Finally we can plot the results with the following parameters:
\begin{itemize}
    \item $nx = ny = 128$
    \item $nt = 100$
    \item $t = 0.005$
\end{itemize}

The output of the serial version is shown in Listing \ref{lst:run_serial}.

\begin{listing}[h]
    \begin{minted}{bash}
$ ./build/main 128 100 0.005
================================================================================
                        Welcome to mini-stencil!
version   :: C++ Serial
mesh      :: 128 * 128 dx = 0.00787402
time      :: 100 time steps from 0 .. 0.005
iteration :: CG 300, Newton 50, tolerance 1e-06
================================================================================
--------------------------------------------------------------------------------
simulation took 0.15112 seconds
1514 conjugate gradient iterations, at rate of 10018.5 iters/second
300 newton iterations
--------------------------------------------------------------------------------
### 1, 128, 100, 1514, 300, 0.15112 ###
Goodbye!
    \end{minted}
    \caption{Running the serial version of the mini-app}
    \label{lst:run_serial}
\end{listing}

The results are shown in Figure \ref{fig:output_serial}.

\begin{figure}[h!t]
    \centering
    \includegraphics[width=0.5\textwidth]{plots/output_serial.png}
    \caption{Results of the nonlinear PDE mini-app}
    \label{fig:output_serial}
\end{figure}


\section{Adding OpenMP to the nonlinear PDE mini-app}

First I reconfigured the project welcome message. If \mintinline{cpp}{_OPENMP} is defined, we use \mintinline{cpp}{omp_get_max_threads()} to get the number of threads. The code welcome message is shown in Listing \ref{lst:openmp_welcome}.

\begin{listing}[h!t]
    \begin{minted}{bash}
#ifdef _OPENMP
    std::cout << "version   :: C++ OpenMP" << std::endl;
    std::cout << "threads   :: " << threads << std::endl;
#else
    std::cout << "version   :: C++ Serial" << std::endl;
#endif
    \end{minted}
    \caption{New OpenMP welcome message}
    \label{lst:openmp_welcome}
\end{listing}


Next I added parallelised versions of the linalg functions. For most of the functions, I simply added the \mintinline{cpp}{#pragma omp parallel for} directive before the loop. For the dot product and the norm functions, I used the \mintinline{cpp}{reduction} clause to ensure the correct result. An example of the copy function is shown in Listing \ref{lst:openmp_copy}.

\begin{listing}[h!t]
    \begin{minted}{cpp}
#pragma omp parallel for shared(y, x)
    for (int i = 0; i < N; i++)
    {
        y[i] = x[i];
    }
    \end{minted}
    \caption{OpenMP copy function}
    \label{lst:openmp_copy}
\end{listing}

Finally, I added the \mintinline{cpp}{#pragma omp parallel for collapse(2)} directive to the stencil operator to parallelise the nested loops. The updated stencil operator is shown in Listing \ref{lst:openmp_stencil}.

\begin{listing}[h!t]
    \begin{minted}{cpp}
#pragma omp parallel for collapse(2)
        for (int j = 1; j < jend; j++)
        {
            for (int i = 1; i < iend; i++)
            {
                f(i, j) = -(4. + alpha) * s_new(i, j) 
                    + s_new(i - 1, j) + s_new(i + 1, j) 
                    + s_new(i, j - 1) + s_new(i, j + 1) 
                    + beta * s_new(i, j) * (1.0 - s_new(i, j)) 
                    + alpha * s_old(i, j);    
            }
        }
    \end{minted}
    \caption{OpenMP stencil operator}
    \label{lst:openmp_stencil}
\end{listing}

\subsection{Bitwise identical results}

Because we use the OpenMP \mintinline{cpp}{reduction} clause in \mintinline{cpp}{hpc_dot} and \mintinline{cpp}{hpc_norm2} we cannot guarantee bitwise identical results. This is because the order of operations is not guaranteed by the standard and can vary between runs. If required be could calculate a tolerance and compare the results within this tolerance. See \href{https://www.openmp.org/spec-html/5.2/openmpsu50.html}{OpenMP docs} for more information.

To test this quickly I calculated a quick checksum of the binary data using the \mintinline{cpp}{std::hash} function. The code is shown in Listing \ref{lst:checksum}. Runnning the programm with different thread counts and comparing the checksums showed that the results are indeed not bitwise identical.

\begin{listing}[h!t]
    \begin{minted}{cpp}
std::size_t hash = std::hash<std::string>{}(
    std::string((char *)f.data(), nx * nx * sizeof(double))
);
    \end{minted}
    \caption{Checksum calculation}
    \label{lst:checksum}
\end{listing}

\subsection{Strong Scaling Benchmark}

To evaluate the strong scaling performance of the OpenMP version I ran it on Euler VII — phase 2 (AMD EPYC 7763) using $CPUs=1,2,4,8,16$ and increasing image size $N=64,128,256,512,1024$. The run configuration I used can be found in the \texttt{code} directory. For each configuration I ran 20 runs, excluded the first (warmup) and then took the average of the remaining times. The strong benchmark results are plotted in Figure \ref{fig:strong_scaling} and the speedup relative to the average serial time is shown in Figure \ref{fig:strong_speedup}.

\begin{figure}[h!t]
    \begin{subfigure}{.5\textwidth}
        \includegraphics[width=\textwidth]{plots/strong_scaling.pdf}
        \caption{Strong scaling benchmark of PDE mini app}
        \label{fig:strong_scaling}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \includegraphics[width=\textwidth]{plots/strong_speedup.pdf}
        \caption{Speedup of PDE mini app}
        \label{fig:strong_speedup}
    \end{subfigure}
    \caption{Strong scaling and speedup of the PDE mini app}
\end{figure}

We can observe a decent speedup for sizes $256$ and up. As expected for all sizes the curve flattens out as we increase the number of threads. A $10$ fold speedup with $16$ threads is achieved for $N=1024$ which is a good result.

The full benchmarking code can be found in the \texttt{code} directory and the resulting data in the \texttt{code/out} directory.

\subsection{Weak Scaling Benchmark}

To evaluate the weak scaling performance of the OpenMP version I ran it on Euler VII — phase 2 (AMD EPYC 7763) using $CPUs=1, 4, 16, 64$ and base image size $N=64, 128, 256$ increasing it to keep the load per CPU constant. For example, for $CPUs=4$ and $N=64$ we would use $N=128$ for $CPUs=16$ and so on. The run configuration I used can be found in the \texttt{code} directory. For each configuration I ran 20 runs, excluded the first (warmup) and then took the average of the remaining times. The weak benchmark results are plotted in Figure \ref{fig:weak_scaling} and the efficiency relative to the average serial time is shown in Figure \ref{fig:weak_efficiency}.

\begin{figure}[h!t]
    \includegraphics[width=\textwidth]{plots/weak_scaling.pdf}
    \caption{Weak scaling benchmark of PDE mini app}
    \label{fig:weak_scaling}
\end{figure}

\begin{figure}[h!t]
    \includegraphics[width=\textwidth]{plots/weak_efficiency.pdf}
    \caption{Efficiency of PDE mini app}
    \label{fig:weak_efficiency}
\end{figure}

For the weak benchmark we would ideally see a flat line. This is however rarely the case and indeed, especially in the beginning we observe a relatively strong time increase (notice the log time scale). For higher thread counts however, the increase is not nearly as strong. A similar picture can be seen in the efficiency as we increase the number of CPUs. Unfortunately for all three $N$ and $CPUs=64$ the variance is quite high and the efficiency results are not reliable. We can however see that we get a sligthly better efficiency for larger $N$. For $CPUs=16$ we still achieve around $14\%$ efficiency with the largest base size.

Again the full benchmarking code can be found in the \texttt{code} directory and the resulting data in the \texttt{code/out} directory.

All source code can be found in \texttt{code/}.
\end{document}
