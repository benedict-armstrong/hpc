\documentclass[unicode,11pt,a4paper,oneside,numbers=endperiod,openany]{scrartcl}

\usepackage{minted}
\usepackage{amsmath}

\input{assignment.sty}

\begin{document}


\setassignment
\setduedate{11 March 2024, 23:59}

\serieheader{High-Performance Computing Lab for CSE}{2024}
{Student: Benedict Armstrong}
{Discussed with: FULL NAME}{Solution for Project 1a}{}
\newline



% \assignmentpolicy

\section{Euler warm-up [10 points]}

\subsection{Module System}
% On the cluster, we provide many centrally installed software and for some software even multiple versions. To configure the environment for a particular software version, we use modules. Modules configure your current computing environment (PATH, LD_LIBRARY_PATH, MANPATH, etc.) to make sure all required binaries and libraries are found.

The module system allows Euler users to quickly and easily configure their environment to use centrally installed software package. A detailed description can be found in the \href{https://scicomp.ethz.ch/wiki/Modules_and_applications}{Module System} documentation.

There are two systems currently in use. The older system is called \texttt{Environment Modules} and the newer system is called \texttt{LMOD Modules}. All new software installations are done with LMOD Modules.

% Code examples
\begin{listing}[!ht]
    \begin{minted}{bash}
    # List all available modules
    module avail
    
    # Load a module
    module load <module_name>
    
    # list all loaded modules
    module list
    \end{minted}
    \caption{Module System}
    \label{lst:module_system}
\end{listing}

\subsection{SLURM}

The Euler cluster uses SLURM to manage and schedule jobs. To run a job on the cluster, you need to submit a job script to the SLURM scheduler. A detailed description can be found in the \href{https://scicomp.ethz.ch/wiki/Job_management_with_SLURM}{SLURM} documentation.

\subsection{Hello Euler!}

We start by compiling and running a simple C program on the Euler cluster. The program is called \texttt{hello\_euler.cpp} and should print "\texttt{Host name: <hostname>}" to standard out.

To run the compiled program on the cluster, we need to submit a job script to the SLURM scheduler. The job script is called \texttt{hello\_euler.slurm} and should look like this:

\begin{listing}[!ht]
    \inputminted{bash}{../01a/hello_euler/hello_euler_1.sh}
    \caption{Job script for running hello\_euler.cpp}
    \label{lst:hello_euler}
\end{listing}

The job can then be submitted to the SLURM scheduler with the following command:

\begin{listing}[!ht]
    \begin{minted}{bash}
    sbatch hello_euler.sh
    \end{minted}
    \caption{Submitting a job to the SLURM scheduler}
    \label{lst:sbatch}
\end{listing}

The code and output can be found in the \texttt{hello\_euler} directory.

\subsection{Multiple Nodes}

We can run the same code on multiple nodes using the following job script:


\begin{listing}[!ht]
    \inputminted{bash}{../01a/hello_euler/hello_euler_2.sh}
    \caption{Job script for running hello\_euler.cpp on multiple nodes}
    \label{lst:hello_euler_2}
\end{listing}

Where we set the number of nodes to 2 and the number of tasks to 2. The output can be found in the \texttt{hello\_euler\_2.out} file.

\section{Performance characteristics [50 points]}

\subsection{Peak performance}

The peak performance of a CPU can be calculated using the following formula:

\begin{equation}
    p_{core} = n_{super} \times n_{FMA} \times f_{SIMD} \times f_{core}
\end{equation}

\begin{equation}
    p_{CPU} = n_{core} \times p_{core}
\end{equation}

in our case this is equal to:

For the EPYC\_7742 node:

\begin{equation}
    p_{core} = 2 \times 2 \times 256 \times 2.25 \times 2.25 = 4095 \text{ GFLOPS}
\end{equation}

\begin{equation}
    p_{CPU} = 64 \times 4095 = 261120 \text{ GFLOPS}
\end{equation}

For the EPYC\_7763 node:

\begin{equation}
    p_{core} = 2 \times 2 \times 256 \times 2.45 \times 2.45 = 4768 \text{ GFLOPS}
\end{equation}

\begin{equation}
    p_{CPU} = 64 \times 4768 = 304192 \text{ GFLOPS}
\end{equation}

\subsection{Memory Hierarchies}

The output of running \texttt{lscpu} and \texttt{hwloc-ls} can be found in the \texttt{memory\_hierarchies} directory. As in the example in the assignment there are also two PDFs detailing the memory hierarchy of the EPYC\_7742 and EPYC\_7763 nodes. In summary both nodes have 8 NUMA nodes, with 8 cores per NUMA node. More information on NUMA can easily be found in the \href{https://en.wikipedia.org/wiki/Non-uniform_memory_access}{Wikipedia} page. Basically it means that the nodes have faster access to their specific part of the shared memory. The rest of the numbers can easily be read out of the two PDFs detailing the memory hierarchy.

\subsubsection{Cache and main memory size}

% Tables for cache (L1-L3) and main memory size for both nodes (EPYC_7742 and EPYC_7763) and if they are shared or not

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline

        \textbf{Cache} & \textbf{EPYC\_7742} & \textbf{EPYC\_7763} \\
        \hline
        L1d            & 32KB                & 32KB                \\
        \hline
        L1i            & 32KB                & 32KB                \\
        \hline
        L2             & 512KB               & 512KB               \\
        \hline
        L3             & 16MB                & 32MB                \\
        \hline
        NUMA           & 63GB                & 31GB                \\
        \hline
        Total Machine  & 502GB               & 248GB               \\
        \hline
    \end{tabular}
    \caption{Cache and main memory size for both nodes}
    \label{tab:cache_main_memory}
\end{table}

\subsection{Bandwidth: STREAM benchmark}

\subsection{Performance model: A simple roofline model}


\end{document}
