\documentclass[unicode,11pt,a4paper,oneside,numbers=endperiod,openany]{scrartcl}

\usepackage{minted}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{listings}

\input{assignment.sty}

\begin{document}


\setassignment
\setduedate{11 March 2024, 23:59}

\serieheader{High-Performance Computing Lab for CSE}{2024}
{Student: Benedict Armstrong}
{Discussed with: -}{Solution for Project 1a}{}
\newline



% \assignmentpolicy

\section{Euler warm-up [10 points]}

\subsection{Module System}
% On the cluster, we provide many centrally installed software and for some software even multiple versions. To configure the environment for a particular software version, we use modules. Modules configure your current computing environment (PATH, LD_LIBRARY_PATH, MANPATH, etc.) to make sure all required binaries and libraries are found.

The module system allows Euler users to quickly and easily configure their environment to use centrally installed software package. A detailed description can be found in the \href{https://scicomp.ethz.ch/wiki/Modules_and_applications}{Module System} documentation.

There are two systems currently in use. The older system is called \texttt{Environment Modules} and the newer system is called \texttt{LMOD Modules}. All new software installations are done with LMOD Modules.

% Code examples
\begin{listing}[!ht]
    \begin{minted}{bash}
    # List all available modules
    module avail
    
    # Load a module
    module load <module_name>
    
    # list all loaded modules
    module list
    \end{minted}
    \caption{Module System}
    \label{lst:module_system}
\end{listing}

\subsection{SLURM}

The Euler cluster uses SLURM to manage and schedule jobs. To run a job on the cluster, you need to submit a job script to the SLURM scheduler. A detailed description can be found in the \href{https://scicomp.ethz.ch/wiki/Job_management_with_SLURM}{SLURM} documentation.

\subsection{Hello Euler!}

We start by compiling and running a simple C program on the Euler cluster. The program is called \texttt{hello\_euler.cpp} and should print "\texttt{Host name: <hostname>}" to standard out.

To run the compiled program on the cluster, we need to submit a job script to the SLURM scheduler. The job script is called \texttt{hello\_euler.slurm} and should look like this:

\begin{listing}[!ht]
    \inputminted{bash}{../01a/hello_euler/hello_euler_1.sh}
    \caption{Job script for running hello\_euler.cpp}
    \label{lst:hello_euler}
\end{listing}

The job can then be submitted to the SLURM scheduler with the following command:

\begin{listing}[!ht]
    \begin{minted}{bash}
    sbatch hello_euler.sh
    \end{minted}
    \caption{Submitting a job to the SLURM scheduler}
    \label{lst:sbatch}
\end{listing}

The code and output can be found in the \texttt{hello\_euler} directory.

\subsection{Multiple Nodes}

We can run the same code on multiple nodes using the following job script:


\begin{listing}[!ht]
    \inputminted{bash}{../01a/hello_euler/hello_euler_2.sh}
    \caption{Job script for running hello\_euler.cpp on multiple nodes}
    \label{lst:hello_euler_2}
\end{listing}

Where we set the number of nodes to 2 and the number of tasks to 2. The output can be found in the \texttt{hello\_euler\_2.out} file.

\section{Performance characteristics [50 points]}

\subsection{Peak performance}

The peak performance of a cluster can be calculated using the following formula:


\begin{align*}
    p_{core}    & = n_{super} \times n_{FMA} \times n_{SIMD} \times f_{core} \\
    p_{CPU}     & = n_{core} \times p_{core}                                 \\
    p_{node}    & = n_{sockets} \times p_{CPU}                               \\
    p_{cluster} & = n_{nodes} \times p_{node}
\end{align*}

The the \textbf{Euler VII — phase 1} and \textbf{Euler VII — phase 2} nodes use the \textit{EPYC\_7H12} and \textit{EPYC\_7763} cpus, respectively.

\begin{table}
    \centering

    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Parameter} & \textbf{Euler VII — phase 1} & \textbf{Euler VII — phase 2} & source                                                                                                \\
        \hline
        CPU                & EPYC\_7H12                   & EPYC\_7763                   & \href{https://scicomp.ethz.ch/wiki/Euler#Euler_VII_.E2.80.94_phase_1}{Euler docs}                     \\
        \hline
        $n_{super}$        & 2                            & 2                            & \href{https://uops.info/table.html}{UOPS Website} ($=TP^{-1}$)                                        \\
        \hline
        $n_{FMA}$          & 2                            & 2                            & \href{https://uops.info/table.html}{UOPS Website}                                                     \\
        \hline
        $n_{SIMD}$         & 4                            & 4                            & \href{https://en.wikichip.org/wiki/amd/microarchitectures/zen_2#Floating_Point_Unit}{en.wikichip.org} \\
        \hline
        $f_{core}$         & 2.6 GHz                      & 2.45 GHz                     & \href{https://scicomp.ethz.ch/wiki/Euler#Euler_VII_.E2.80.94_phase_1}{Euler docs}                     \\
        \hline
        $n_{core}$         & 64                           & 64                           & \href{https://scicomp.ethz.ch/wiki/Euler#Euler_VII_.E2.80.94_phase_1}{Euler docs}                     \\
        \hline
        $n_{sockets}$      & 2                            & 2                            & \href{https://scicomp.ethz.ch/wiki/Euler#Euler_VII_.E2.80.94_phase_1}{Euler docs}                     \\
        \hline
        $n_{nodes}$        & 292                          & 248                          & \href{https://scicomp.ethz.ch/wiki/Euler#Euler_VII_.E2.80.94_phase_1}{Euler docs}                     \\
        \hline
    \end{tabular}
    \caption{Parameters of the Euler VII — phase 1 and Euler VII — phase 2 nodes}
    \label{tab:peak_performance}
\end{table}

Using the values from the table \ref{tab:peak_performance} we can calculate the peak performance of the Euler VII — phase 1 and Euler VII — phase 2 nodes.
\\

Euler VII — phase 1:
\begin{align*}
    p_{core}    & = 2 \times 2 \times 4 \times 2.6 \, \text{GHz}  =     41.6 \, \text{GFLOPS}               \\
    p_{CPU}     & = 64 \times 41.6 \, \text{GFLOPS}                  =  2662.4 \, \text{GFLOPS}             \\
    p_{node}    & = 2 \times 2662.4 \, \text{GFLOPS}                =   5324.8 \, \text{GFLOPS}             \\
    p_{cluster} & = 292 \times 5324.8 \, \text{GFLOPS}               =  \underline{1554.8 \, \text{TFLOPS}}
\end{align*}

Euler VII — phase 2:
\begin{align*}
    p_{core}    & = 2 \times 2 \times 4 \times 2.45 \, \text{GHz}  = 39.2 \, \text{GFLOPS}               \\
    p_{CPU}     & = 64 \times 39.2 \, \text{GFLOPS}                = 2508.8 \, \text{GFLOPS}             \\
    p_{node}    & = 2 \times 2508.8 \, \text{GFLOPS}               = 5017.6 \, \text{GFLOPS}             \\
    p_{cluster} & = 248 \times 5017.6 \, \text{GFLOPS}             = \underline{1244.4 \, \text{TFLOPS}}
\end{align*}

\subsection{Memory Hierarchies}

The output of running \texttt{lscpu} and \texttt{hwloc-ls} can be found in the \texttt{memory\_hierarchies} directory. As in the example in the assignment there are also two PDFs detailing the memory hierarchy of the EPYC\_7H12 and EPYC\_7763 nodes. Both nodes have 8 NUMA nodes, with 8 cores per NUMA node. More information on NUMA can easily be found in the \href{https://en.wikipedia.org/wiki/Non-uniform_memory_access}{Wikipedia} page. Basically it means that the nodes have faster access to their specific part of the shared memory. The rest of the numbers can easily be read out of the two PDFs detailing the memory hierarchy. The main difference between the two nodes is the size of the L3 cache, which is 16MB for the EPYC\_7H12 and 32MB for the EPYC\_7763. The main memory size is the same for both nodes at 248GB.

\subsubsection{Cache and main memory size}

% Tables for cache (L1-L3) and main memory size for both nodes (EPYC_7H12 and EPYC_7763) and if they are shared or not

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline

        \textbf{Cache} & \textbf{EPYC\_7H12}             & \textbf{EPYC\_7763}             \\
        \hline
        L1d            & 32KB                            & 32KB                            \\
        \hline
        L1i            & 32KB                            & 32KB                            \\
        \hline
        L2             & 512KB                           & 512KB                           \\
        \hline
        L3 (shared)    & \textcolor{red}{16MB} (4 cores) & \textcolor{red}{32MB} (8 cores) \\
        \hline
        NUMA           & 31GB                            & 31GB                            \\
        \hline
        Total Machine  & 248GB                           & 248GB                           \\
        \hline
    \end{tabular}
    \caption{Cache and main memory size for both nodes}
    \label{tab:cache_main_memory}
\end{table}


\subsection{Bandwidth: STREAM benchmark}

As per the STREAM benchmark documentation we need to set DSTREAM\_ARRAY\_SIZE to be a four times the L3 cache size.

For the EPYC\_7H12 node the cache size (\ref{tab:cache_main_memory}) is 16MB. $$16\text{MB} = 1.6e7\text{B}$$ So we set DSTREAM\_ARRAY\_SIZE to be $1.6e7 \times 4 / 8 + 2e6 = 10e6$ (we add a little extra as recommended).

\begin{lstlisting}
Function    Best Rate MB/s  Avg time     Min time     Max time
Copy:           25125.0     0.006416     0.006368     0.006472
Scale:          18591.8     0.008643     0.008606     0.008667
Add:            19702.7     0.012235     0.012181     0.012269
Triad:          20075.2     0.012017     0.011955     0.012046
\end{lstlisting}

The peak bandwidth for the EPYC\_7763 node is around $20\text{GB/s}$. \\

The same calculation for the EPYC\_7763 node gives us a DSTREAM\_ARRAY\_SIZE of $32\text{MB} \times 4 / 8 + 4e6 = 20e6$.

\begin{lstlisting}
Function    Best Rate MB/s  Avg time     Min time     Max time
Copy:           35390.3     0.009231     0.009042     0.009746
Scale:          25082.3     0.012878     0.012758     0.013321
Add:            25780.4     0.018783     0.018619     0.019019
Triad:          26000.8     0.018654     0.018461     0.019720
\end{lstlisting}

The peak bandwidth for the EPYC\_7763 node is around $25\text{GB/s}$. \\

The entire output of the STREAM benchmark for both CPUs can be found in the \texttt{stream\_benchmark} directory.

\subsection{Performance model: A simple roofline model}


Using the STREAM benchmark results and the peak performance of the CPUs we can create a simple roofline model. The peak performance of the EPYC\_7H12 and EPYC\_7763 nodes are $41.6\text{GFLOPS}$ and $39.2\text{GFLOPS}$, respectively. The peak bandwidth for the EPYC\_7H12 and EPYC\_7763 nodes are $20\text{GB/s}$ and $25\text{GB/s}$, respectively.

% insert image of roofline model (../01a/roofline_model/roofline.png)
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{../01a/roofline_model/roofline.png}
    \caption{Roofline model for the EPYC\_7H12 and EPYC\_7763 nodes}
    \label{fig:roofline}
\end{figure}

The ridge point for the EPYC\_7H12 is around $I_{ridge}=2$, and for the EPYC\_7763 node is around $I_{ridge}=1.6$.

\section{Auto-vectorization}

\begin{enumerate}
    \item Why is it important for data structures to be aligned?

          It is important for data structures to be aligned because the CPU can only load and store data from memory in chunks of a certain size. If the data is not aligned, the CPU will have to load and store the data in multiple chunks, which is less efficient. For example, if the CPU can load 128 bits at a time, and the data is not aligned, the CPU will have to load 64 bits, then 64 bits, which is less efficient than loading 128 bits at once.

    \item What are some obstacles that can prevent automatic vectorization by the compiler?

          Some obstacles that can prevent automatic vectorization by the compiler are:
          \begin{itemize}
              \item The code is not written in a way that the compiler can understand and optimize.
              \item The code contains dependencies that prevent the compiler from reordering instructions.
              \item Complex loop terminations
              \item The code contains loops that the compiler cannot unroll.
              \item Using pointers instead of arrays
          \end{itemize}

    \item Is there a way to help the compiler to vectorize and how?

          Yes, there are ways to help the compiler to vectorize. For example, you can use compiler directives to give the compiler hints about how to vectorize the code. You should also write code in a way that the compiler can understand and optimize, for example by using simple loops, avoiding dependencies and using arrays instead of pointers.

    \item Which loop optimizations are performed by the compiler to vectorize and pipeline loops?

          The compiler can perform several loop optimizations to vectorize and pipeline loops. For example, the compiler can unroll loops to expose more instruction-level parallelism, and it can reorder instructions to eliminate dependencies. The compiler can also use loop interchange to improve data locality, and it can use loop fusion to combine multiple loops into a single loop. The compiler can also use loop tiling to break up large loops into smaller loops that fit in the cache.

    \item What can be done if automatic vectorization by the compiler fails or is sub-optimal?

          If automatic vectorization by the compiler fails or is sub-optimal, you can try to rewrite the code in a way that the compiler can understand and optimize (See previous points). You can also use compiler directives to give the compiler hints about how to vectorize the code. Examples are \texttt{\#pragma ivdep} and \texttt{\#pragma vector align}.

\end{enumerate}

\section{Matrix multiplication optimization}

The goal of this task was to improve the performance of a simple matrix multiplication program. To do this I implemented blocked matrix multiplication.

\inputminted[firstline=19, lastline=40]{c}{../01b/dgemm-blocked.c}

One important parameter is the block size. To evaluate different block sizes I added a macro \texttt{BLOCK\_SIZE} to the code.

\inputminted[firstline=6, lastline=9]{c}{../01b/dgemm-blocked.c}

Plotting the performance of the blocked matrix multiplication for different block sizes we see that we get the best performance for a block size of around 10. This result was not expected, as the L1 cache size is 32KB, which using the calculation from the task description $$ 32\text{KB} = 32e3\text{B} \rightarrow 32e3\text{B} / 8 = 4000\,\text{doubles} \rightarrow \sqrt{\frac{4000}{3}} = 36,51$$ would give us a theoretical ideal block size of 36.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{../01b/BLOCKSIZE_RUNS/blocksize.pdf}
    \caption{Performance of blocked matrix multiplication for different block sizes}
    \label{fig:blocked_matrix_multiplication}
\end{figure}

The entire output of the blocked matrix multiplication for different block sizes can be found in the \texttt{BLOCKSIZE\_RUNS} directory.

\end{document}
