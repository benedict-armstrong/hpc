\documentclass[unicode,11pt,a4paper,oneside,numbers=endperiod,openany]{scrartcl}

\usepackage{listings}

\input{assignment.sty}

\begin{document}


\setassignment
\setduedate{11 March 2024, 23:59}

\serieheader{High-Performance Computing Lab for CSE}{2024}
{Student: Benedict Armstrong}
{Discussed with: FULL NAME}{Solution for Project 1a}{}
\newline

% \assignmentpolicy

\section{Euler warm-up [10 points]}

\subsection{Module System}
% On the cluster, we provide many centrally installed software and for some software even multiple versions. To configure the environment for a particular software version, we use modules. Modules configure your current computing environment (PATH, LD_LIBRARY_PATH, MANPATH, etc.) to make sure all required binaries and libraries are found.

The module system allows Euler users to quickly and easily configure their environment to use centrally installed software package. A detailed description can be found in the \href{https://scicomp.ethz.ch/wiki/Modules_and_applications}{Module System} documentation.

There are two systems currently in use. The older system is called \texttt{Environment Modules} and the newer system is called \texttt{LMOD Modules}. All new software installations are done with LMOD Modules.

% Code examples
\begin{lstlisting}[language=bash,caption={Euler module system}]
    # List all available modules
    module avail

    # Load a module
    module load <module_name>

    # list all loaded modules
    module list
\end{lstlisting}

\subsection{SLURM}

The Euler cluster uses SLURM to manage and schedule jobs. To run a job on the cluster, you need to submit a job script to the SLURM scheduler. A detailed description can be found in the \href{https://scicomp.ethz.ch/wiki/Job_management_with_SLURM}{SLURM} documentation.

\subsection{Hello Euler!}

We start by compiling and running a simple C program on the Euler cluster. The program is called \texttt{hello\_euler.cpp} and should print "\texttt{Host name: <hostname>}" to standard out.

To run the compiled program on the cluster, we need to submit a job script to the SLURM scheduler. The job script is called \texttt{hello\_euler.slurm} and should look like this:

\begin{lstlisting}[language=bash,caption={Hello Euler Job Script}]
    #!/bin/bash
    #SBATCH --job-name=hello_euler      # Job name    (default: sbatch)
    #SBATCH --output=hello_euler.out # Output file (default: slurm-%j.out)
    #SBATCH --error=hello_euler.err  # Error file  (default: slurm-%j.out)
    #SBATCH --time=00:01:00               # Wall clock time limit
    #SBATCH --nodes=1                    # Number of tasks
    #SBATCH --ntasks=1                    # Number of tasks
    #SBATCH --cpus-per-task=1             # Number of CPUs per task
    #SBATCH --mem-per-cpu=1024            # Memory per CPU
    #SBATCH --constraint=EPYC_9654

    srun hello_euler
\end{lstlisting}

The job can then be submitted to the SLURM scheduler with the following command:
\begin{lstlisting}[language=bash]
    sbatch hello_euler.sh
\end{lstlisting}

The code and output can be found in the \texttt{hello\_euler} directory.

\section{Performance characteristics [50 points]}

\subsection{Peak performance}

\subsection{Memory Hierarchies}

\subsubsection{Cache and main memory size}

\subsection{Bandwidth: STREAM benchmark}

\subsection{Performance model: A simple roofline model}


\end{document}
