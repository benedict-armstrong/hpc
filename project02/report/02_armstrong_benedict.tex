\documentclass[unicode,11pt,a4paper,oneside,numbers=endperiod,openany]{scrartcl}

\usepackage{minted}
\usepackage{amsmath}
\usepackage{listings}

\input{assignment.sty}

\begin{document}


\setassignment
\setduedate{25 March 2024, 23:59}

\serieheader{High-Performance Computing Lab for CSE}{2024}
{Student: Benedict Armstrong}
{Discussed with: Tristan Gabl}{Solution for Project 02}{}
\newline

% \assignmentpolicy

\section{Computing $\pi$ with \texttt{OpenMP} [20 points]}

We used two different ways to parallelize the computation of $\pi$. Defining a parallel region using the \texttt{\#pragma parallel} directive and the calculating the work for each thread \ref{lst:pi_parallel_critical}. The second method is to use the \texttt{\#pragma parallel for} directive to parallelize the for loop. The benchmark results are shown in Figure \ref{fig:pi_benchmark}.

\begin{listing}[h!t]
    \begin{minted}{c}
    #pragma omp parallel
    {
        double sum_private = 0.;
        int nthreads = omp_get_num_threads();
        int tid = omp_get_thread_num();
        int i_beg = tid * N / nthreads;
        int i_end = (tid + 1) * N / nthreads;
        for (int i = i_beg; i < i_end; ++i)
        {
        double x = (i + 0.5) * h;
        sum_private += 4.0 / (1.0 + x * x);
        }

    #pragma omp critical
        sum += sum_private;
    }
    \end{minted}
    \caption{Parallel computation of $\pi$ using \texttt{OpenMP} and a parallel region.}
    \label{lst:pi_parallel_critical}
\end{listing}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{../code/pi/benchmark.png}
    \caption{Benchmark results for the $\pi$ calculation using \texttt{OpenMP}.}
    \label{fig:pi_benchmark}
\end{figure}

\section{The Mandelbrot set  using \texttt{OpenMP} [20 points]}

The Mandelbrot set can be parallelized by splitting the image into $N$ equal parts, where $N$ is the number of threads. Each thread then calculates the Mandelbrot set for its assigned part of the image. The pragma directive for parallelizing the Mandelbrot set calculation is shown in Listing \ref{lst:mandel}. The result of a strong scaling benchmark (using $1000$ iterations to determine convergence) is shown in Figure \ref{fig:mandel_benchmark}. The generated Mandelbrot set image is shown in Figure \ref{fig:mandel}.

\begin{listing}[h!t]
    \begin{minted}{c}
#pragma omp parallel for shared(pPng) \ 
    private(i, j, x, y, x2, y2, cx, cy) reduction(+ : nTotalIterationsCount)
    \end{minted}
    \caption{Pragma directive for parallelizing the Mandelbrot set calculation.}
    \label{lst:mandel}
\end{listing}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{../code/mandel/benchmark.png}
    \caption{Benchmark results for the Mandelbrot set calculation using \texttt{OpenMP}.}
    \label{fig:mandel_benchmark}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{../code/mandel/images/mandel_seq_1000.png}
    \caption{Mandelbrot set}
    \label{fig:mandel}
\end{figure}

\section{Bug hunt [10 points]}

\begin{enumerate}
    \item The first bug is a compile-time bug. The \texttt{\#pragma} directive must be followed by a for loop. In this case we have a \texttt{tid = omp\_get\_thread\_num()} statement immediately after the \texttt{\#pragma}. This is not allowed. The fix is to move the \texttt{tid} assignment into the for loop.
    \item There are a couple of errors in the code. The variable \texttt{tid} should be made explicitly private as every thread is writing to it. In the last \texttt{for} loop the total sum should be marked as a reduction variable (using the \texttt{reduction(+:total)} clause). Also by default the second loop will not spawn any new threads as the option \texttt{OMP\_NESTED} is set to \texttt{FALSE} by default (see \href{https://www.ibm.com/docs/en/xl-c-aix/13.1.2?topic=openmp-omp-nested}{IBM OpenMP documentation}).

\end{enumerate}

\section{Parallel histogram calculation using \texttt{OpenMP} [15 points]}

The histogram calculation can be easily parallelized using a one line compiler directive. The code is shown in Listing \ref{lst:hist}. The strong scaling benchmark results are shown in Figure \ref{fig:hist_benchmark} and the speedup results are shown in Figure \ref{fig:hist_speedup}.

\begin{listing}[h!t]
    \begin{minted}{c}
        #pragma omp parallel for reduction(+ : dist[ : BINS])
    \end{minted}
    \caption{Parallel histogram calculation using \texttt{OpenMP}}
    \label{lst:hist}
\end{listing}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{../code/hist/benchmark.png}
    \caption{Benchmark results for the histogram calculation using \texttt{OpenMP}.}
    \label{fig:hist_benchmark}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{../code/hist/speedup.png}
    \caption{Speedup results for the histogram calculation using \texttt{OpenMP}.}
    \label{fig:hist_speedup}
\end{figure}

\section{Parallel loop dependencies with \texttt{OpenMP} [15 points]}

To parallelize the loop with dependencies we split up the loop into $N$ equal parts, where $N$ is the number of threads. We then calculate the first element of each thread's partition
$$S_i = Sn * {up}^{i * chunk\_size}$$
where $i$ is the thread number. Each thread then calculates the rest of the assigned elements. Figure \ref{fig:loop_dependencies_benchmark} shows the benchmark results and Figure \ref{fig:loop_dependencies_speedup} shows the speedup results.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{../code/loop-dependencies/benchmark.png}
    \caption{Benchmark results for the loop dependencies calculation using \texttt{OpenMP}.}
    \label{fig:loop_dependencies_benchmark}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{../code/loop-dependencies/speedup.png}
    \caption{Speedup results for the loop dependencies calculation using \texttt{OpenMP}.}
    \label{fig:loop_dependencies_speedup}
\end{figure}

\section{Quicksort using \texttt{OpenMP} tasks [20 points]}

Quicksort can be easily parallelized using tasks. We create a task for each recursive call to the quicksort function. We then wait for all tasks to finish before returning. This is done by adding a \texttt{taskwait} directive after the recursive calls. The only complication is defining a minimum task size to prevent the creation of too many tasks. The code is shown in Listing \ref{lst:quicksort_tasks}.

% Code examples
\begin{listing}[h!t]
    \begin{minted}{c}
        #pragma omp task shared(data) firstprivate(right) final(right < MIN_SIZE)
        quicksort(data, right);
      
        int t = length - left;
      #pragma omp task shared(data, left) firstprivate(t) final(t < MIN_SIZE)
        quicksort(&(data[left]), t);
      
      #pragma omp taskwait
    \end{minted}
    \caption{Recursion of the quicksort function using tasks}
    \label{lst:quicksort_tasks}
\end{listing}

After implementing the quicksort function using tasks I benchmarked the code using a strong scaling analysis. The results are shown in Figure \ref{fig:quicksort_benchmark}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{../code/quicksort/benchmark.png}
    \caption{Benchmark results for the quicksort calculation using \texttt{OpenMP} tasks.}
    \label{fig:quicksort_benchmark}
\end{figure}



\end{document}
